# Adaptive-RAG
首先回顾两种RAG系统的重要变体，CRAG和Self-RAG。
1、CRAG（Corrective Retrieval Augmented Generation）
   CRAG，即纠正性检索增强生成，主要关注于在检索阶段后引入一个额外的知识精炼（Knowledge Correction）环节。
   这个环节会对检索到的每一个相关切片（snippets）进行评估，判断其是否对问题有效。如果评估结果为不正确（Incorrect），则会**丢弃原来的文档**，并可能通过**网络搜索**获取新的相关信息。
   CRAG的特色在于其自我评估和自我修正的能力，旨在提高生成结果的准确性和质量。
2、Self-RAG（Self-Reflective Retrieval Augmented Generation）
   Self-RAG，即**自我反思式**检索增强生成，强调通过自我反思能力来调整自身的检索和生成结果。
   它会在生成内容的过程中进行自我评分，以识别和减少可能出现的幻觉（即与事实不符的生成内容）。
   1️⃣**Self-RAG的流程从问题判断开始，根据需要选择是否进行检索；**
   2️⃣并对检索到的文档进行评估和排序，检索到的文档是否能支撑生成内容。
   3️⃣最后送入语言模型进行生成，生成的内容进行评估，判断是否结束响应或者重新生成。
   其重点在于通过反思评分来优化检索和生成过程，提高整体性能。
3、Adaptive-RAG（Adaptive Retrieval-Augmented Generation）
   Adaptive-RAG是一种更加灵活和动态的RAG变体，它根据查询的复杂度自适应地选择最合适的检索策略。
   这包括**使用一个小型的语言模型作为分类器**来预测查询的复杂度，并根据复杂度级别选择相应的检索策略（如迭代式、单步式或无检索方法）。
   Adaptive-RAG的核心思想在于通过动态调整来平衡处理简单查询和复杂查询时的效率和准确性。
   接下来重点讲诉Adaptive-RAG。
   论文：Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity
   Code：github.com/starsuzi/Adaptive-RAG
   Adaptive-RAG 方案能够自适应地根据 user query 的难度选择合适的 RAG 模型来解决。比如对于直接性的 query，就可以直接让 LLM 回答；
                      对于简单的 query，只需要一轮 retrieval 就可以让 LLM 完成回答；而对于复杂的需要 multi-hop 的 query，则需要多轮 retrieval 才能让 LLM 完成回答。

    论文概要
    在论文中，作者指出了以下两个观察到的现象：
    用户大多数的问题都是简单的问题，少数情况下才是需要多条推理的复杂问题简单的问题使用复杂的 RAG 模型存在开销的浪费，而复杂的问题又无法用简单的 RAG 模型来解决为此，
       本论文才提出了如下的方案（最右边的 C 就是本论文提出的 Adaptive-RAG 的思路）：
    如上图，在 C 所示的思路中，存在一个 classifier 来对 user query 做困难度分类，然后再交由**三种用于处理不同复杂的 RAG 模型**的其中一个来完成解决。

    实现细节
    本工作将 user query 的难度分为了三种，并给出了三种对应的解决策略：
     Non Retrieval for QA：对于 Straightforward Query，不需要经过检索，直接由 LLM 回答即可。
     Single-step Approach for QA：对于 Simple Query，只需要经过一轮检索即可获取支持 LLM 回复的 doc。
     Multi-step Approach for QA：对于 Complex Query，需要多步、复杂的检索才能得到答案。

     对于一个具体的 user query，如何将其分类呢？这里就是训练了一个小语言模型作为 classifier，输入是 user query，输出是 query 的难度。
     原论文使用了 T5-large 并再训练得到的 classifier。但是，目前并没有可用的 query-complexity pairs 数据集，因此，论文介绍了该工作是如何收集到用于训练 classifier 数据集的。
     query-complexity paris 是借助于已有的 QA 数据集来构建，为已有的 QA 数据集的 pair 标注 complexity label 来构建本实验所需的数据集。该数据集的收集主要包括两个过程：

     1、Generate silver data from predicted outcomes of models：意思是说，假如难度分成 [A, B, C] 三个等级，A 最简单，C 最困难，
        那么给定一个 query，首先先让 LLM 直接回答，如果 LLM 回答正确，则将 query 标记为 A；如果 LLM 经过一轮检索后生成正确答案，则将 query 标记为 B；如果 LLM 经过多轮检索后生成正确答案，则将 query 标记为 C。
    2、Utilize inductive bias in datasets：经过第一个过程，有些 query 仍然无法被标记，因为可能三种 RAG 模型都没有生成正确答案，这个时候只能利用这个过程来完成标注。
      这时候利用一个特点：这些 benchmark datasets 的数据都有一定的偏向性，比如一个 dataset 可能都比较偏 single-hop，而另一个 dataset 可能就都比较偏 multi-hop，
       对于偏 single-hop 的则直接标为 B，否则直接标为 C。
     经过以上两个过程，我们的数据集就构建出来了。

      补充讨论
     多次查询确实是有问题的，有些查询可能并不需要那么多，可能是多余的甚至是反效果的，
     本文的自适应确实是有一定收益。新增一种划分策略的思路，可以通过难度来划分应对策略，这个不仅在处理性能上有收益，在最终效果上也有收益。
     分类这块，可以看到这个分类的效果确实比较差，个人感觉原因主要是在**分类问题的定义**上，**难度**和大模型、和知识库支持、和问题领域之类的差异会比较大，
    本身分类效果不好应该是意料之中，不过好奇是这个分类器的优化会给RAG整体效果带来多大收益仍未可知。
     self-rag被放进来进行对比，结果发现非常拉胯，有些让人出乎意外。感觉这里有打开方式、适应场景等的问题，有展开分析的价值。


# ReAct 与 CoT、ToT 的区别
当下提高大模型推理能力的几个主要技术，从CoT（Chain of Thought）到TOT（Tree of Thought），再到ReAct。

## CoT（Chain of Thought，思维链）
第一次接触到CoT是在Prompt工程中，其作为Prompt高级技巧的一部分，可以显著提高大模型在推理方面的能力，尤其是解决数学等具有逻辑性的问题时。
区别于传统的 Prompt 从输入直接到输出的映射 <input——>output> 的方式，CoT 完成了从输入到思维链再到输出的映射，即 <input——>reasoning chain——>output>。
例如，如果问题是“纽约到洛杉矶的距离是多少？”，模型可能首先检索纽约和洛杉矶的坐标，然后计算两点之间的距离，最后给出答案。
在这个过程中，模型不仅提供了答案，还展示了其推理过程，增强了答案的可信度。原论文：https://arxiv.org/pdf/2201.11903.pdf[48]

关于使用COT：在现在应该是大模型本身就应该具备的能力，只需要在Prompt 中添加了一句 “Let's Step by Step” 就让大模型在推理上用到了思维链。然后使用就可以了，并不需要自己去写代码写逻辑去亲自实现CoT。

Self-consistency with CoT（CoT的自我一致性）
一种CoT在实际应用中的方案是：Self-consistency with CoT（CoT的自我一致性）。简单地要求模型对同一提示进行多次回答，并将多数结果作为最终答案。
它是CoT（Chain of Thought）的后续方法。示例代码如下：对同一个Prompt，重复调用5次，然后取其中多数结果作为最终答案。

ToT（Tree of Thought，思维树）
在CoT的基础上，有人指出其存在的缺陷：
对于局部，没有探索一个思考过程下的不同延续——树的分支。对于全局，没有利用任何类型的规划，前瞻以及回溯去帮助评估不同抉择——而启发式的探索正是人类解决问题的特性。
针对以上缺陷，他提出了ToT（Tree of Thought，思维树）的概念。原论文：https://arxiv.org/pdf/2305.10601.pdf[49]具体结构图如下：虚线左边为基本Prompt、CoT以及CoT Self Consisitency，虚线右边为ToT。

从原始输入开始，在思维链的每一步，采样多个分支。逐渐形成树状结构，对每一个分支进行评估，然后在合适的分支上进行搜索。这就是ToT思维树的基本过程。
它以树状图的形式展示思考的层次和分支。在决策制定、问题解决、创意生成等场景中，思维树可以帮助人们系统地探索各种可能性，评估不同选项，从而做出更明智的决策。

在思维树中：
1、根节点：通常代表问题或决策的起点，即需要解决的核心问题。
2、分支：从根节点开始，每个分支代表一个可能的思考方向或解决方案。分支可以进一步细分，形成更详细的子分支，代表更具体的思考步骤或子问题。
3、叶节点：树的末端，代表思考过程的最终结果或结论。

通过构建思维树，人们可以：
1、系统地探索确保所有可能的思考方向都被考虑，避免遗漏重要的信息或解决方案。
2、评估和比较：通过比较不同分支的结果，评估各种选项的优劣，做出更合理的决策。
3、增强理解：通过可视化思考过程，增强对问题的理解，使复杂的决策过程变得清晰。
目前针对TOT我们还没有得到特别好的效果，可能是在构建当中还有不合理的定义或者解析问题不精准的存在。但从对于资源的合理性投入，供应链的管理，提高决策质量和效率它应该是有天然的优势存在。


# ReAct   （Agent前身？）
个人理解一下：CoT、ToT 都是作用在大模型本身的内在推理（Reason）过程上，而 ReAct 则是统筹整个系统，从推理过程，结合外部工具共同实现最终的目标（Reason + Action）。
原论文：https://arxiv.org/pdf/2210.03629[50]
以ReAct论文中那张图来看，可以更清晰的理解ReAct与CoT、ToT的区别

对于ReAct这个框架可以理解为是一种结合了推理和行动的新型人工智能框架，主要用于**增强AI系统在复杂环境中的决策能力和执行效率**。
ReAct框架的核心思想是通过实时检索相关信息和执行基于这些信息的行动，来辅助AI系统进行更准确的推理和决策。
在ReAct框架中，AI系统不仅依赖于其预训练的知识，还会在遇到新情况时，主动检索外部信息（如数据库、网络资源等），并将这些信息整合到其决策过程中。
这一过程可以看作是AI系统在“思考”（Reasoning）和“行动”（Acting）之间的循环，其中：
1、思考（Reasoning）：AI系统基于当前状态和目标，进行推理和规划，确定下一步需要采取的行动或需要检索的信息。
2、行动（Acting）：根据推理结果，AI系统执行相应的行动，如检索信息、执行任务等。
3、反馈：AI系统根据行动的结果，更新其状态和知识，然后再次进入思考阶段，形成一个闭环。

ReAct框架的优势在于，它使AI系统能够适应不断变化的环境，处理之前未见过的情况，而不仅仅是依赖于预训练数据。
通过实时检索和整合新信息，AI系统可以做出更准确、更灵活的决策，提高其在复杂任务中的表现。

# 总结
CoT目前来看已经集成进了大模型内部，通过在Prompt中加入一些提示词（Let's think step by step）即可唤醒大模型的CoT思考能力。
对于ToT，有实现代码参考，就是在思维链的基础上，每一步不再是只有一个结果，而是采样多个分支，综合评估。
对于ReAct，则是从推理过程，结合外部工具共同实现最终目标。ReAct不止在推理，还在利用外部工具实现目标，Reason + Action，而Cot、ToT 则只是 Reason。

# Chunking优化
整理自：
Chunking Strategies for LLM Applications | Pinecone[51]
https://blog.csdn.net/EnjoyEDU/article/details/141038019[52]
高级 RAG 技术学习笔记 - aneasystone's blog[53]
几乎所有的大模型或嵌入模型，输入长度都是受限的，因此，需要将文档进行分块，通过分块可以确保嵌入的内容尽可能少地包含噪音，同时嵌入内容和用户查询之间具有更高的语义相关性。
有很多种不同的分块策略，比如按长度进行分割，保证每个分块大小适中；也可以按句子或段落进行分割，防止将完整的句子切成两半；
每种分块策略可能适用于不同的情况，需要仔细斟酌这些策略的优点和缺点，确定他们的适用场景，这篇博客[54]对常见的分块策略做了一个总结。

文本切割的主要观点
文本切割是优化语言模型应用性能的关键步骤。切割策略应根据数据类型和语言模型任务的性质来定制。
传统的基于物理位置的切割方法（如字符级切割和递归字符切割）虽简单，但可能无法有效地组织语义相关的信息。
语义切割和基因性切割是更高级的方法，它们通过分析文本内容的语义来提高切割的精确度。
使用多向量索引可以提供更丰富的文本表示，从而在检索过程中提供更相关的信息。多向量索引是一种将文本数据以多个向量的形式进行表示和索引的技术。这些向量可能来源于不同的向量化技术或算法，但更重要的是它们能够共同反映文本的多维度特征和信息。
工具和技术的选择应基于对数据的深入理解以及最终任务的具体需求。

文本切割的主要策略
1. 固定大小分块（Fixed-size chunking）
。字符级切割，简单粗暴地按字符数量固定切割文本。
这是最常见也是最直接的分块策略，文档被分割成固定大小的分块（chunk_size），分块之间可以保留一些重叠（chunk_overlap），以确保不会出现语义相关的内容被不自然地拆分的情况。
在大多数情况下，固定大小分块都是最佳选择，与其他形式的分块相比，它既廉价又简单易用，而且不需要使用任何自然语言处理库。
分块大小是一个需要深思熟虑的参数，它取决于你所使用的嵌入模型的 token 容量，比如，基于 BERT 的sentence-transformer最多只能处理 512 个 token，而 OpenAI 的ada-002能够处理 8191 个；
另外这里也需要权衡大模型的 token 限制，由于分块大小直接决定了我们加载到大模型上下文窗口中的信息量，这篇博客[55]中对不同的分块大小进行了实验，可以看到不同的分块大小可以得到不同的性能表现。

。递归字符切割： 考虑文本的物理结构，如换行符、段落等，逐步递归切割。它可以接受一组分隔符，比如["\n\n", "\n", " ", ""]，
它首先使用第一个分隔符对文本进行分块，如果第一次分块后长度仍然超出分块大小，则使用第二个，以此类推，通过这种递归迭代的过程，直到达到所需的块大小。
大模型的上下文限制是 token 数量，而不是文本长度，因此当我们将文本分成块时，建议计算分块的 token 数量，比如使用 OpenAI 的 tiktoken[56]库。

2. 句子拆分（Sentence splitting）
很多模型都针对句子级内容的嵌入进行了优化，所以，如果我们能将文本按句子拆分，可以得到很好的嵌入效果。
常见的句子拆分方法有下面几种：直接按英文句号（.）、中文句号（。）或换行符等进行分割。
这种方法快速简单，但这种方法不会考虑所有可能的边缘情况，类似上面字符级切割的递归字符切割可能会破坏句子的完整性。
NLTK 是一个流行的自然语言工具包，它提供了一个句子分词器（sentence tokenizer），可以将文本分割成句子。
spaCy 是另一个强大的用于自然语言处理任务的 Python 库，它提供了复杂的句子分割功能，可以高效地将文本分割成单独的句子，从而在生成的块中更好地保留上下文。

3. 特定格式分块（Specialized chunking）
文档特定切割，有很多文本文件具有特定的结构化内容，比如 Markdown、LaTeX、HTML 或 各种源码文件等，针对这种格式的内容可以使用一些专门的分块方法。
Markdown：LangChain 的MarkdownHeaderTextSplitter[57]就是基于这一想法实现的分块方法，它通过 Markdown 的标题来组织分组，然后再在特定标题组中创建分块。
HTML：LangChain 中的HTMLHeaderTextSplitter[58]根据标题来实现 HTML 的分块，HTMLSectionSplitter[59]能够在元素级别上分割文本，它基于指定的标签和字体大小进行分割，将具有相同元数据的元素组合在一起，以便将相关文本语义地分组，并在文档结构中保留丰富的上下文信息。
LaTeX：通过解析 LaTeX 可以创建符合内容逻辑组织的块（例如章节、子章节和方程式），从而产生更准确和上下文相关的结果。LangChain 的LatexTextSplitter实现了 LaTex 格式的分块。
JSON：需要考虑嵌套的 JSON 对象的完整性，通常按照深度优先的方式遍历 JSON 对象，并构建出较小的 JSON 块，参考 LangChain 的RecursiveJsonSplitter[60]。
PDF：使用pypdf按照page逐页解析pdf；使用 pyplumber 将pdf逐页进行解析， 但是文本结构在分栏的时候存在混淆，解析不完全；使用 PDFMiner ，将整个文档解析成一个完整的文本。文本结构可以自行认为定义；使用非结构化 Unstructured；

4. 语义分块（Semantic chunking）
利用语言模型的嵌入向量来分析文本的意义和上下文，以确定切割点。这是一种实验性地分块技术，最初由 Greg Kamradt 提出，
它在The 5 Levels Of Text Splitting For Retrieval[61]这个视频中将分块技术划分为 5 个等级，其中语义分块（Semantic chunking）是第 4 级。
它的基本原理如下：首先将文本划分成一个个句子，并计算第一个句子的向量；接着计算第二个句子的向量，并和第一个句子进行比较，得到相似度；
接着计算第三个句子的向量，并和第二个句子进行比较，得到相似度，以此类推；当句子之间的相似度“显著下降”或“变化超过某个阈值”时，认为是话题的转折点，可以作为分块的切点。
这里[62]是对应的代码实现。LangChain 的SemanticChunker[63]和 LlamaIndex 的SemanticSplitterNodeParser[64]都实现了语义分块。

5. 智能切割
创建一个Agent，使用Agent来决定如何切割文本，以更智能地组织信息。


# 句子窗口检索
在介绍句子窗口检索之前，我们先简单介绍一下普通的 RAG 检索，下面是普通 RAG 检索的流程：
先将文档切片成大小相同的块--------将切片后的块进行 Embedding 并保存到向量数据库--------根据问题检索出 Embedding 最相似的 K 个文档库-----将问题和检索结果一起交给 LLM 生成答案。
普通 RAG 检索的问题是如果文档切片比较大的话，检索结果可能会包含很多无关信息，从而导致 LLM 生成的结果不准确。

我们再来看下句子窗口检索的流程：和普通 RAG 检索相比，句子窗口检索的文档切片单位更小，
通常以句子为单位在检索时，除了检索到匹配度最高的句子，还将该句子周围的上下文也作为检索结果一起提交给 LLM父文档检索一节中，
我们提到，通过检索更小的块可以获得更好的搜索质量，然后通过扩大上下文范围可以获取更好的推理结果。
句子窗口检索使用的也是这个思想，句子窗口检索让检索内容更加准确，同时上下文窗口又能保证检索结果的丰富性。

原理
句子窗口检索的原理其实很简单，首先在文档切分时，将文档以句子为单位进行切分，一句话相比于一段话来说，语义可能要更接近于用户的问题，同时进行 Embedding 并保存数据库。
然后在检索时，通过问题检索到相关的句子，但并不只是将检索到的句子作为检索结果，而是将该句子前面和后面的句子一起作为检索结果放到一个窗口中，
窗口包含的句子数量可以通过参数来进行设置，最后将检索结果再一起提交给 LLM 来生成答案。可以看到整个过程和父文档检索几乎是一样的，
但是 LlamaIndex 为了区别其实现方式，将其放在了后处理模块，而不是检索模块。


# Embedding发展
有很多嵌入模型可供选择，比如 BAAI 的bge-large[65]，微软的multilingual-e5-large[66]，OpenAI 的text-embedding-3-large[67]等，可以在MTEB 排行榜[68]上了解最新的模型更新情况。
词嵌入技术经历了一个从静态到动态的发展过程[69]，静态嵌入为每个单词使用单一向量，而动态嵌入根据单词的上下文进行调整，可以捕获上下文理解。排行榜上排名靠前的基本上都是动态嵌入模型。
此外，关于嵌入模型的优化，通常围绕着**嵌入模型的微调**展开，将嵌入模型定制为特定领域的上下文，特别是对于术语不断演化或罕见的领域，可以参考下面的一些教程：
Training and Finetuning Embedding Models with Sentence Transformers v3[70]
Using LangSmith to Support Fine-tuning[71]

值得一提的是，嵌入不仅仅限于文本，我们还可以创建图像或音频的嵌入，并将其与文本嵌入进行比较，这个概念适用于强大的图像或音频搜索、分类、描述等系统。

# RAG效果评估(具体再看)
RAGAS：
Evaluating RAG pipelines with Ragas + LangSmith[72]、AI大模型探索之路-应用篇11：AI大模型应用智能评估（Ragas）-CSDN博客[73]
TruLens：使用 Trulens 评估 RAG 应用[74]


# 开发 RAG 系统面临的 12 个问题
Paper：https://arxiv.org/pdf/2401.05856[75]在论文中，作者提出了7个 RAG 系统中可能会面临的问题：
1. 缺失内容（Missing Content）
   当用户的问题无法从文档库中检索到时，可能会导致大模型的幻觉现象。理想情况下，RAG 系统可以简单地回复一句 “抱歉，我不知道”，
   然而，如果用户问题能检索到文档，但是文档内容和用户问题无关时，大模型还是可能会被误导。
2. 错过超出排名范围的文档（Missed Top Ranked）   ！重排
   由于大模型的上下文长度限制，我们从文档库中检索时，一般只返回排名靠前的 K 个段落，如果问题答案所在的段落超出了排名范围，就会出现问题。
3. 不在上下文中（Not In Context）
   包含答案的文档已经成功检索出来，但却没有包含在大模型所使用的上下文中。当从数据库中检索到多个文档，并且使用合并过程提取答案时，就会出现这种情况。
   答案在提供的上下文中，但是大模型未能准确地提取出来，这通常发生在上下文中存在过多的噪音或冲突信息时。
5. 错误的格式（Wrong Format）
   问题要求以特定格式提取信息，例如表格或列表，然而大模型忽略了这个指示。
6. 不正确的具体性（Incorrect Specificity）
   尽管大模型正常回答了用户的提问，但不够具体或者过于具体，都不能满足用户的需求。不正确的具体性也可能发生在用户不确定如何提问，或提问过于笼统时。
7. 不完整的回答（Incomplete Answers）
   考虑一个问题，“文件 A、B、C 包含哪些关键点？”，直接使用这个问题检索得到的可能只是每个文件的部分信息，导致大模型的回答不完整。
   一个更有效的方法是分别针对每个文件提出这些问题，以确保全面覆盖。
Wenqi Glantz 在他的博客12 RAG Pain Points and Proposed Solutions[76]中又扩充了另 5 个问题：
8. 数据摄入的可扩展性问题（Data Ingestion Scalability）
   当数据规模增大时，系统可能会面临如数据摄入时间过长、系统过载、数据质量下降以及可用性受限等问题，这可能导致性能瓶颈甚至系统故障。
9. 结构化数据的问答（Structured Data QA）
   根据用户的问题准确检索出所需的结构化数据是一项挑战，尤其是当用户的问题比较复杂或比较模糊时。
   这是由于文本到 SQL 的转换不够灵活，当前大模型在处理这类任务上仍然存在一定的局限性。
   复杂的 PDF 文档中可能包含有表格、图片等嵌入内容，在对这种文档进行问答时，传统的检索方法往往无法达到很好的效果。
   我们需要一个更高效的方法来处理这种复杂的 PDF 数据提取需求。
11. 备用模型（Fallback Model(s)）
    在使用单一大模型时，我们可能会担心模型遇到问题，比如遇到 OpenAI 模型的访问频率限制错误。这时候，我们需要一个或多个模型作为备用，以防主模型出现故障。
12. 大语言模型的安全性（LLM Security）
    如何有效地防止恶意输入、确保输出安全、保护敏感信息不被泄露等问题，都是我们需要面对的重要挑战。
在 Wenqi Glantz 的博客中，他不仅整理了这些问题，而且还对每个问题给出了对应的解决方案，整个 RAG 系统的蓝图如下：


# 上下文压缩
在问题优化，检索到的文档以及生成的答案阶段都可以进行压缩。
1、问题优化阶段，将聊天历史压缩成最终问题以便检索。
2、检索阶段，不要立即按原样返回检索到的文档，可以对查询到的上下文对其进行压缩。
3、答案生成阶段，对生成的答案进行压缩。


# 敏感信息处理
检索的文档中可能含有如用户名、身份证、手机号等敏感信息，这类信息统称为PII（Personal Identifiable Information、个人可识别信息），
如果将这类信息丢给大模型生成回复，可能存在一定的安全风险，所以需要在后处理步骤中将 PII 信息删除。
LlamaIndex 提供了两种方式来删除 PII 信息[77]：
使用大模型（PIINodePostprocessor）和使用专用的NER 模型（NERPIINodePostprocessor）。


# 引用来源
一个基于 RAG 的应用不仅要提供答案，还要提供答案的引用来源，这样做有两个好处，
首先，用户可以打开引用来源对大模型的回复进行验证，
其次，方便用户对特定主体进行进一步的深入研究。
这里是Perplexity 泄露出来的 Prompt[78]可供参考，
这里是WebLangChain 对其修改后的实现[79]。
在这个 Prompt 中，要求大模型在生成内容时使用[N]格式表示来源，然后在客户端解析它并将其呈现为超链接。
