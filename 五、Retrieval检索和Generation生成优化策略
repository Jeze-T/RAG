Retrieval检索优化策略
  
完成对问题的改写、不同知识库查询的构建以及路由分发、查询构建和索引生成优化之后可以进一步优化Retrieval检索。
包括Ranking、Refinement以及Adaptive retrival。

Retrieval检索
首先需要先检索，之后再进行后面的Ranking、Refinement和Adaptive retrival操作。
先来说一下检索：检索是在索引的基础上进行查询的，所以检索方式和索引结构分不开。
构建索引的目的是为了更快的检索，检索器可以【针对单个索引】，也可以组合不同检索技术，主要有以下的几种类型：

  
1. 父文档检索（Parent Document Retrieval）（单文档较长、信息密度低））
当我们对文档进行分块的时候，我们可能希望每个分块不要太长，因为只有当文本长度合适，嵌入才可以最准确地反映它们的含义，太长的文本嵌入可能会失去意义；
但是在将检索内容送往大模型时，我们又希望有足够长的文本，以保留完整的上下文。
为了实现二者的平衡，有以下三种方式实现父文档检索：

可以在检索过程中，首先获取小的分块，然后查找这些小分块的父文档，并返回较大的父文档，
这里的父文档指的是小分块的来源文档，可以是整个原始文档，也可以是一个更大的分块。

使用大模型对文档进行摘要，然后对摘要进行嵌入和检索，这种方法对处理包含大量冗余细节的文本非常有效，这里的原始文档就相当于摘要的父文档。【在四的part12中有使用】
  
通过大模型为【每个文档】生成假设性问题（Hypothetical Questions），【在二的part9中有使用】然后对问题进行嵌入和检索，也可以结合问题和原文档一起检索，这种方法提高了搜索质量，因为与原始文档相比，用户查询和假设性问题之间的语义相似性更高。
  
  
2. 层级检索（Hierarchical Retrieval）（大量文档、需快速缩小搜索范围））
有大量的文档需要检索，为了高效地在其中找到相关信息，一种高效的方法是创建两个索引：一个由摘要组成，另一个由文档块组成，
然后分两步搜索，首先通过摘要筛选出相关文档，然后再在筛选出的文档中搜索。【在part四中有提到，RAPTOR是其中的一种实现方式】

  
3. 混合检索（Fusion Retrieval）

【在二的part 6】有提到RAG 融合（RAG Fusion）技术，它根据用户的原始问题生成意思相似但表述不同的子问题并检索。
其实，还可以结合不同的检索策略，最常见的做法是将基于关键词的老式搜索和基于语义的现代搜索结合起来。
  
基于关键词的搜索又被称为稀疏检索器（sparse retriever），通常使用BM25[22]、TF-IDF[23]等传统检索算法；
基于语义的搜索又被称为密集检索器（dense retriever），使用的是现在流行的 Embedding 算法。

通常结合了稀疏检索(Sparse Retrieval)和稠密检索(Dense Retrieval)的策略，通常可以兼顾两种检索方式的优势，提高检索的效果和效率。
两种方法的详细解释如下：
稀疏检索(Sparse Retrieval)：
这种方法通常基于倒排索引(Inverted Index)，对文本进行词袋(Bag-of-Words)、BM25或者TF-IDF表示，然后按照关键词的重要性对文档进行排序。
稀疏检索的优点是速度快，可解释性强，但在处理同义词、词语歧义等语义问题时效果有限。
  
稠密检索(Dense Retrieval)：这种方法利用深度神经网络，将查询和文档映射到一个低维的稠密向量空间，然后通过向量相似度(如点积、余弦相似度)来度量查询与文档的相关性。
稠密检索能更好地捕捉语义信息，但构建向量索引的成本较高，检索速度也相对较慢。
  
关键词搜索和向量搜索各有其优势和局限性：
特性                    向量搜索                              关键词搜索（全文搜索）
语义理解                ✔️ 能理解查询的语义和上下文          ❌ 仅基于字面匹配
同义词处理               ✔️ 自然处理同义词和近义词            ❌ 需要额外配置
精确匹配                  ❌ 不擅长精确匹配                  ✔️ 擅长精确匹配关键词和短语
数字和ID查询             ❌ 表现较差                        ✔️ 表现出色
专有名词处理             ❌ 可能无法准确表示                  ✔️ 能准确匹配
跨语言搜索                ✔️ 一定程度支持                    ❌ 通常局限于单一语言
长文本和复杂查询          ✔️ 表现良好                       ❌ 可能难以处理
结构化数据处理             ❌ 不如全文搜索高效                  ✔️ 高效处理
布尔逻辑支持？               ❌ 通常不支持                     ✔️ 支持复杂布尔查询
计算效率                  ❌ 计算成本较高                    ✔️ 通常更快、更高效
拼写错误处理               ✔️ 较为宽容                        ❌ 较为敏感（可通过模糊匹配改善）

在实际RAG系统的开发中，现实通常是各种情况都有，难以使用一种搜索方法解决全部问题。
用户的查询可能涵盖广泛的类型，从精确的关键词匹配到抽象的概念探索，再到专业领域的术语搜索。
同时，知识库中的数据也可能是多样化的，包含结构化和非结构化信息、数字数据、专有名词等。
面对这些复杂的需求，仅依赖向量搜索或全文搜索中的一种往往会导致检索结果的不准确。
这就是为什么在现代RAG系统中，混合搜索方法变得越来越重要的原因。

混合搜索的工作原理：
1、并行执行：
对每个查询，系统同时执行向量搜索和全文搜索。
向量搜索捕捉查询的语义内容。全文搜索处理关键词匹配和精确查找。
2、结果融合：
使用特定算法将两种搜索的结果合并成一个统一的结果集。
最常用的方法之一是倒数排名融合（Reciprocal Rank Fusion，RRF）算法。
并行执行的实现并不复杂，它的关键技巧是结果融合，这个问题通常是通过倒数排名融合（Reciprocal Rank Fusion，RRF）算法来解决的，RRF 算法对检索结果重新进行排序从而获得最终的检索结果。

RRF 是滑铁卢大学和谷歌合作开发的一种算法，它可以将具有不同相关性指标的多个结果集组合成单个结果集，这里是它的论文地址[24]，
其中最关键的部分就是下面这个公式： 
RRF的主要机制是根据每个检索结果的排名位置来分配权重，权重的计算公式为：
其中：d是文档或项。n是检索系统的数量，即有多少个检索器的结果被用来融合。是文档在第个检索器中的排名（rank），越靠前的排名值越小。k是一个常数，通常取60，用来防止低排名项的权重过高。
  
k 是一个常量，默认值为 60。RRF 不依赖于每次检索分配的绝对分数，而是依赖于相对排名，这使得它非常适合组合来自可能具有不同分数尺度或分布的查询结果。
实现权重平衡的办法，可以通过直接调整经典RRF公式中的k值来实现。在经典RRF公式中，K为常数，建议设为60。
实际这个K是可调的，通过调整k值，我们可以有效地改变关键词搜索和向量搜索的相对重要性权重，从而使RRF算法获得更好的性能。

直接修改k值是对RRF公式增加权重平衡的最简单方法，易于实施和调整，适合快速优化和实验。
  
  
2023年5月，来自向量数据库初创企业Pinecone和伯克利的研究人员共同发表了论文，提出了一种新的混合搜索算法，称为TM2C2（Theoretical Min-Max Convex Combination），
论文中，我们看到TM2c2算法有如下几个优势：
稳定性：相比传统的 min-max 归一化，TM2C2 更稳定。
性能：在大多数数据集上，TM2C2 优于 RRF 和其他基线方法。
可解释性：α 参数直观地表示了语义搜索和关键词搜索的相对重要性。
样本效率：只需要很少的训练样本就能调整到较好的性能。
我们可以发现，TM2c2算法实际上是RRF引入权重参数和归一函数后的变体。这一变化为特定场景下，混合搜索的性能提升提供了更多的可能性。


4.多向量检索（Multi-Vector Retrieval）
对于同一份文档，可以有多种嵌入方式，也就是为同一份文档生成几种不同的嵌入向量，这在很多情况下可以提高检索效果，这被称为多向量检索器（Multi-Vector Retriever）。
为同一份文档生成不同的嵌入向量有很多策略可供选择，上面所介绍的父文档检索就是比较典型的方法。
当我们处理包含文本和表格的半结构化文档时，多向量检索器也能派上用场，在这种情况下，可以提取每个表格，为表格生成适合检索的摘要，但生成答案时将原始表格送给大模型。
有些文档不仅包含文本和表格，还可能包含图片，随着多模态大模型的出现，我们可以为图像生成摘要和嵌入。
LangChain 的这篇博客[25]对多向量检索做了一个全面的描述，并提供了大量的示例，用于表格或图片等多模任务的检索。
注意：父文档检索和层级检索很相似，其区别在于父文档检索只检索一次摘要，然后由摘要扩展出原始文档，而层级检索是通过检索摘要筛选出一批文档，然后在筛选出的文档中执行二次检索。

5. 后处理
RAG 系统的最后一个问题，如何将检索出来的信息丢给大模型？
检索出来的信息可能过长，或者存在冗余（比如从多个来源进行检索），我们可以在后处理步骤中对其进行**上下文压缩、排序、去重**等。
LangChain 中并没有专门针对后处理的模块，文档也是零散地分布在各个地方，比如Contextual compression[26]、Cohere reranker[27]等。


  
15-Re-ranking（**扩大召回分为两步，先增加召回数量，就是增加k的大小，再从这k个召回里面进行重排，再从重排的召回中选择top-k**）
检索得到的数据直接提交给LLM去生成答案，但这样存在检索出来的chunks并不一定完全和上下文相关的问题，最后导致大模型生成的结果质量不佳。
这个问题很大程度上是因为**召回相关性不够或者是召回数量太少**导致的，
从扩大召回这个角度思考，借鉴推荐系统做法，引入粗排或重排的步骤来改进效果。
  
重排越来越popular，在上面的过滤策略中，我们经常会用到 Embedding 来计算文档的相似性，然后根据相似性来对文档进行排序（包括Fusion），
这里的排序被称为粗排，我们还可以使用一些专门的排序引擎对文档进一步排序和过滤，这被称为 **精排。
**每个子问题检索到的文档根据设定的权重进行排序（每个子问题都用到的文档权重更高）。再基于这个权重，在选择top-k的文档。

解决召回数量太少的方法是原有的top-k向量检索召回扩大召回数目；
Rerank主要有两种实现方式：使用一些专门的排序引擎对文档进一步排序和过滤使用大模型来做重排序。

1. 使用Cohere 的Re-Rank方案
We can also useCohere Re-Rank[28].
Seehere[29]:

除此之外，排序引擎还有JinaRerank 、SentenceTransformerRerank、Colbert Reranker：
Jina AI[30]总部位于柏林，是一家领先的 AI 公司，提供一流的嵌入、重排序和提示优化服务，实现先进的多模态人工智能。可以使用 Jina 提供的Rerank API[31]来对文档进行精排。
  
除了使用商业服务，我们也可以使用一些本地模型来实现重排序。比如sentence-transformer[32]包中的交叉编码器（Cross Encoder）可以用来重新排序节点。
LlamaIndex 默认使用的是cross-encoder/ms-marco-TinyBERT-L-2-v2模型，这个是速度最快的。为了权衡模型的速度和准确性，请参考sentence-transformer 文档[33]，以获取更完整的模型列表。
  
另一种实现本地重排序的是ColBERT[34]模型，它是一种快速准确的检索模型，可以在几十毫秒内对大文本集合进行基于 BERT 的搜索。

  
2. 大模型做重排序
使用大模型来做重排序，将文档丢给大模型，然后让大模型对文档的相关性进行评分，从而实现文档的重排序。
使用 LLM 来决定哪些文档/文本块与给定查询相关。prompt由一组候选文档组成，这时LLM 的任务是选择相关的文档集，并用内部指标对其相关性进行评分。
为了避免因为大文档chunk化带来的内容分裂，在建库阶段也可做一定优化，利用summary index对大文档进行索引。下面是 LlamaIndex 内置的用于重排序的 Prompt：

基于LLM召回或重排存在一些缺陷，首先就是慢，第二就是增加了LLM的调用成本，第三，由于打分是分批进行的，存在着无法全局对齐的问题。
除此之外，使用LLM进行排序的相关论文方法还有：
RankGPT 是 Weiwei Sun 等人在论文Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents[35]中提出的一种基于大模型的 zero-shot 重排方法，
它采用了排列生成方法和滑动窗口策略来高效地对段落进行重排序，具体内容可以参考RankGPT 的源码[36]。
RankLLM[37]和 RankGPT 类似，也是利用大模型来实现重排，只不过它的重点放在与FastChat[38]兼容的开源大模型上，比如 Vicuna 和 Zephyr 等，
并且对这些开源模型专门为重排任务进行了微调，比如 RankVicuna 和 RankZephyr 等。



16-Retrieval (CRAG)
其本质上是一种Adaptive-RAG策略，实现方式为在循环单元测试中**自我纠正检索错误**，以确定文档相关性并返回到网络搜索，
即纠对检索文档的自我反思/自我评分，主要采用如下步骤：

首先需要知道的是CRAG的特色发生在retrieval阶段的最后，即当我们获得到了近似的document（或者说relevant snippets）之后。
然后会进入一个额外的环节，叫Knowledge Correction。
在这里呢我们会先对retrieval得到的每一个相关切片snippets进行evaluate，评估一下我们获取到的snippet是不是对问的问题有效？
（此处重点：evaluator也是一个LLM）然后会有三种情况：
Correct：那就直接进行RAG的正常流程。（不过图中是加了进一步的优化）
Incorrect：那就直接丢弃掉原来的document，直接去web里搜索相关信息
Ambiguous：对于模糊不清的，就两种方式都要
  
那么在最后的generation部分，也是根据三种不同的情况分别做处理。
之前是correct，那现在就直接拼接问题和相关文档
之前是incorrect，那现在就直接拼接问题和web获取的信息
之前是ambiguous，那现在就拼接三个加起来
  
以上是CRAG的原始大概逻辑，但在langchain中对此进行了简化：
在Langchain中只存在两种情况，即：
当incorrect的时候，直接就去web上search了（先经过一个transform_query对问题进行重写，变成更适合web搜索的形式）。
  
如果至少有一个文档超过了相关性阈值（correct），则进入生成阶段。
在生成之前，进行知识细化将文档划分为知识条带(knowledge strip)， 对每个知识条进行分级，过滤不相关的知识条，
如果所有文档都低于相关性阈值，或者分级者不确定，那么框架就会寻找额外的数据源，并使用网络搜索来补充检索。

更准确地说是工程简化 + 默认策略化。
LangChain的优化并不是算法上更聪明，而是：
简化流程逻辑，降低使用门槛
提供默认的评分器、阈值、重写器和 Web 工具链（如 Tavily 等）
通过 pipeline 把 CRAG 的思想串成自动流程，用户不需要手动判别 ambiguous 了
  
  
Deep Dive
https://www.youtube.com/watch?v=E2shqsYwxck[39]
Notebooks
https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_crag.ipynb[40]
https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph\_crag\_mistral.ipynb[41]
Self-Reflective RAG with LangGraph[42]




Generation生成优化策略
17-Retrieval (Self-RAG)
和CRAG的核心一样，都是self-reflective，即当发现结果不是那么有效时，要通过环回溯到之前的步骤去优化。
其基本思想在于：使用**循环单元测试**自行纠正RAG错误，以检查文档相关性、答案幻觉和答案质量。

和CRAG不一样的是，selfRAG的流程是从最开始进行的，评估三次，
大概流程：首先先判断问题**是不是需要retrieval**，如上图右下角，
此处的问题是写一篇essay，那其实根本没必要去retrieval，直接放入LLM就行
当问题需要检索的时候，我们会将得到的每个document snippet分别判断，**是否检索到了有关relevant**：
如果无关，那就不进行下一步。
如果有关，那**是否支持support 生成相关内容**，或者部分支持partial support，或者不支持。
当我们对所有snippets都判断后，按照相关性进行排序，然后依次送到LLM中去进行最后的步骤。在最后生成后还有一次评估，**评估生成的内容是否是有效的**，总共三次。

在Langchain中的self-RAG的流程图如上图所示，流程如下：
检索阶段判断：
Self-RAG 首先确定**检索到的文档是否与问题相关**。将检索到的信息添加到响应是否有帮助。相关如果是，它会发出一个检索过程的信号，并要求外部检索模块找到相关文档。
生成阶段判断：
如果不需要检索，Self-RAG 会像常规语言模型一样预测响应的下一个部分。
如果需要检索，它首先**评估检索到的文档是否支持生成内容**（能否基于检索到的文档生成内容），然后根据所发现的内容生成响应的下一个部分。
最终结果判断：
如果检索到的文档能支持生成内容，则生成内容。并且评估**生成响应的整体质量**。如果整体质量好，则结束本次响应，否则，再次从头开始检索。
  
Notebooks
langgraph/examples/rag at main · langchain-ai/langgraph · GitHub[43]
https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph\_self\_rag\_mistral\_nomic.ipynb[44]
https://arxiv.org/pdf/2310.11511[45]
